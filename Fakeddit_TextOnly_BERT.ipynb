{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fakeddit 文本‑only（BERT）基线\n",
        "\n",
        "本 Notebook 以 **Fakeddit v2.0 的多模态样本（仅文本）** 为输入，构建论文方法论中的**文本分支**基线。\n",
        "\n",
        "**目标**\n",
        "- 使用 `clean_title` 训练文本模型（BERT）\n",
        "- 支持 2/3/6‑way 分类\n",
        "- 为后续多模态融合提供可对照的文本基线\n",
        "\n",
        "**你将依次完成**\n",
        "1. 配置与依赖\n",
        "2. 数据加载与标签映射\n",
        "3. BERT 训练与评估\n",
        "4. 保存模型与后续扩展建议\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 0. 环境准备（必要时执行）\n",
        "\n",
        "如果你本地未安装依赖，可以运行：\n",
        "```bash\n",
        "pip install torch torchvision torchaudio transformers scikit-learn tqdm\n",
        "```\n",
        "\n",
        "说明：首次运行会下载 HuggingFace 模型权重（`bert-base-uncased`）。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 0.1 Linux 服务器环境建议（与你的配置一致）\n",
        "\n",
        "你给的环境是 **PyTorch 2.8.0 / Python 3.12 / Ubuntu 22.04 / CUDA 12.8**，\n",
        "建议直接使用项目根目录的 `requirements.txt`：\n",
        "```bash\n",
        "pip install -r requirements.txt\n",
        "```\n",
        "\n",
        "如果在服务器上运行，请确保：\n",
        "- `nvidia-smi` 可正常输出 GPU\n",
        "- CUDA 驱动版本 ≥ 12.8\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# 环境检查（可选）\n",
        "# - 确认 PyTorch / CUDA / GPU 是否正常\n",
        "\n",
        "import torch\n",
        "print('torch:', torch.__version__)\n",
        "print('cuda available:', torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print('cuda version:', torch.version.cuda)\n",
        "    print('gpu:', torch.cuda.get_device_name(0))\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# 配置区：集中管理所有实验超参数与路径\n",
        "# - 修改 TASK 可切换 2/3/6-way\n",
        "# - 修改 MODEL_NAME 可切换不同 BERT 变体\n",
        "# - MAX_SAMPLES 用于快速小样本调试\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "DATA_ROOT = Path('Fakeddit datasetv2.0')\n",
        "TRAIN_PATH = DATA_ROOT / 'multimodal_only_samples' / 'multimodal_train.tsv'\n",
        "VAL_PATH = DATA_ROOT / 'multimodal_only_samples' / 'multimodal_validate.tsv'\n",
        "TEST_PATH = DATA_ROOT / 'multimodal_only_samples' / 'multimodal_test_public.tsv'\n",
        "\n",
        "TASK = 2  # 2 / 3 / 6\n",
        "MODEL_NAME = 'bert-base-uncased'\n",
        "MAX_LEN = 128\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 3\n",
        "LR = 2e-5\n",
        "WEIGHT_DECAY = 0.01\n",
        "WARMUP_RATIO = 0.1\n",
        "MAX_SAMPLES = None\n",
        "NUM_WORKERS = 2\n",
        "\n",
        "OUTPUT_DIR = Path('outputs') / f'bert_text_only_{TASK}way'\n",
        "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print('Train:', TRAIN_PATH)\n",
        "print('Val  :', VAL_PATH)\n",
        "print('Test :', TEST_PATH)\n",
        "print('Output:', OUTPUT_DIR)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# 依赖与随机种子\n",
        "# - 保证实验可复现\n",
        "# - 自动选择 GPU / CPU\n",
        "\n",
        "import csv\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, get_linear_schedule_with_warmup\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
        "\n",
        "try:\n",
        "    from tqdm import tqdm\n",
        "except Exception:\n",
        "    # 如果 tqdm 未安装，给一个简单兜底\n",
        "    def tqdm(x, **kwargs):\n",
        "        return x\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('device:', device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 1. 标签映射（已在本地 TSV 验证）\n",
        "- 2‑way：`1=True`，`0=Fake`\n",
        "- 3‑way：`0=True`，`1=Fake with true text`，`2=Fake with false text`\n",
        "- 6‑way：`0=True`，`1=Satire/Parody`，`2=False Connection`，`3=Imposter`，`4=Manipulated`，`5=Misleading`\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# 加载 TSV 并抽取 clean_title + 标签\n",
        "# - 使用 clean_title 保证文本预处理与论文一致\n",
        "# - 可通过 MAX_SAMPLES 做小样本调试\n",
        "\n",
        "LABEL_NAMES = {\n",
        "    2: {0: 'Fake', 1: 'True'},\n",
        "    3: {0: 'True', 1: 'Fake-TrueText', 2: 'Fake-FalseText'},\n",
        "    6: {0: 'True', 1: 'Satire/Parody', 2: 'False Connection', 3: 'Imposter', 4: 'Manipulated', 5: 'Misleading'},\n",
        "}\n",
        "\n",
        "def load_tsv(path, task=2, max_samples=None):\n",
        "    texts = []\n",
        "    labels = []\n",
        "    label_key = f'{task}_way_label'\n",
        "    with open(path, newline='', encoding='utf-8') as f:\n",
        "        reader = csv.DictReader(f, delimiter='\t')\n",
        "        for row in reader:\n",
        "            text = (row.get('clean_title') or '').strip()\n",
        "            if not text:\n",
        "                continue\n",
        "            label_str = row.get(label_key)\n",
        "            if label_str is None or label_str == '':\n",
        "                continue\n",
        "            label = int(float(label_str))\n",
        "            texts.append(text)\n",
        "            labels.append(label)\n",
        "            if max_samples is not None and len(texts) >= max_samples:\n",
        "                break\n",
        "    return texts, labels\n",
        "\n",
        "train_texts, train_labels = load_tsv(TRAIN_PATH, task=TASK, max_samples=MAX_SAMPLES)\n",
        "val_texts, val_labels = load_tsv(VAL_PATH, task=TASK, max_samples=MAX_SAMPLES)\n",
        "test_texts, test_labels = load_tsv(TEST_PATH, task=TASK, max_samples=MAX_SAMPLES)\n",
        "\n",
        "print('train:', len(train_texts))\n",
        "print('val  :', len(val_texts))\n",
        "print('test :', len(test_texts))\n",
        "\n",
        "from collections import Counter\n",
        "print('train label dist:', Counter(train_labels))\n",
        "print('val label dist  :', Counter(val_labels))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 2. 分词与数据集封装\n",
        "\n",
        "这一部分把文本转换成 BERT 可以读取的 token，并封装成 PyTorch 数据集。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# 分词器初始化\n",
        "# - 自动加载与 MODEL_NAME 对应的 tokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    # 数据集封装：提供 __len__ 和 __getitem__\n",
        "    def __init__(self, texts, labels):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.texts[idx], self.labels[idx]\n",
        "\n",
        "def collate_fn(batch):\n",
        "    # 批量分词 + padding + truncation\n",
        "    texts, labels = zip(*batch)\n",
        "    enc = tokenizer(list(texts), padding=True, truncation=True, max_length=MAX_LEN, return_tensors='pt')\n",
        "    enc['labels'] = torch.tensor(labels, dtype=torch.long)\n",
        "    return enc\n",
        "\n",
        "train_ds = TextDataset(train_texts, train_labels)\n",
        "val_ds = TextDataset(val_texts, val_labels)\n",
        "test_ds = TextDataset(test_texts, test_labels)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, collate_fn=collate_fn)\n",
        "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, collate_fn=collate_fn)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 3. 模型、损失函数与优化器\n",
        "\n",
        "这里创建 BERT 分类器，并加入类别不平衡的权重。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# 模型初始化\n",
        "# - num_labels 决定分类头的类别数\n",
        "\n",
        "num_labels = TASK\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=num_labels)\n",
        "model.to(device)\n",
        "\n",
        "# 类别权重：用于缓解类别不平衡（尤其是 6-way）\n",
        "from collections import Counter\n",
        "label_counts = Counter(train_labels)\n",
        "weights = [0.0] * num_labels\n",
        "total = sum(label_counts.values())\n",
        "for k in range(num_labels):\n",
        "    count = label_counts.get(k, 1)\n",
        "    weights[k] = total / (num_labels * count)\n",
        "weights = torch.tensor(weights, dtype=torch.float, device=device)\n",
        "\n",
        "import torch.nn as nn\n",
        "criterion = nn.CrossEntropyLoss(weight=weights)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
        "total_steps = len(train_loader) * EPOCHS\n",
        "warmup_steps = int(total_steps * WARMUP_RATIO)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)\n",
        "\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=torch.cuda.is_available())\n",
        "print('total steps:', total_steps, 'warmup:', warmup_steps)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 4. 评估函数\n",
        "\n",
        "输出 loss / accuracy / macro‑F1 / 混淆矩阵。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# 评估函数\n",
        "# - 统一评估逻辑，方便在 val/test 上复用\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    total_loss = 0.0\n",
        "    for batch in loader:\n",
        "        labels = batch.pop('labels').to(device)\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        outputs = model(**batch)\n",
        "        logits = outputs.logits\n",
        "        loss = criterion(logits, labels)\n",
        "        total_loss += loss.item() * labels.size(0)\n",
        "        preds = torch.argmax(logits, dim=-1)\n",
        "        all_preds.extend(preds.cpu().tolist())\n",
        "        all_labels.extend(labels.cpu().tolist())\n",
        "\n",
        "    acc = accuracy_score(all_labels, all_preds)\n",
        "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "    avg_loss = total_loss / max(1, len(all_labels))\n",
        "    return avg_loss, acc, f1, cm\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 5. 训练\n",
        "\n",
        "训练过程中每个 epoch 会输出验证集指标。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# 训练循环\n",
        "# - 使用 AMP 提升速度\n",
        "# - 每个 epoch 评估一次验证集\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for batch in tqdm(train_loader, desc=f'Epoch {epoch}/{EPOCHS}'):\n",
        "        labels = batch.pop('labels').to(device)\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        optimizer.zero_grad()\n",
        "        with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
        "            outputs = model(**batch)\n",
        "            logits = outputs.logits\n",
        "            loss = criterion(logits, labels)\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        scheduler.step()\n",
        "        running_loss += loss.item() * labels.size(0)\n",
        "\n",
        "    train_loss = running_loss / max(1, len(train_ds))\n",
        "    val_loss, val_acc, val_f1, val_cm = evaluate(model, val_loader)\n",
        "    print(f'Epoch {epoch}: train_loss={train_loss:.4f} val_loss={val_loss:.4f} val_acc={val_acc:.4f} val_f1={val_f1:.4f}')\n",
        "    print('Val confusion matrix:\\n', val_cm)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 6. 测试集评估与保存\n",
        "\n",
        "最后在测试集上评估并保存模型。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# 测试集评估与保存\n",
        "# - 测试集仅用于最终报告\n",
        "\n",
        "test_loss, test_acc, test_f1, test_cm = evaluate(model, test_loader)\n",
        "print(f'Test: loss={test_loss:.4f} acc={test_acc:.4f} f1={test_f1:.4f}')\n",
        "print('Test confusion matrix:\\n', test_cm)\n",
        "\n",
        "# 保存模型与 tokenizer\n",
        "model.save_pretrained(OUTPUT_DIR)\n",
        "tokenizer.save_pretrained(OUTPUT_DIR)\n",
        "print('Saved to', OUTPUT_DIR)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 7. 后续多模态扩展建议\n",
        "- 添加图像分支（ResNet/ViT）获取 `image_emb`\n",
        "- 加入语义一致性分支（`cosine` / MLP）\n",
        "- 融合策略：`concat(text_emb, image_emb, consistency)` + MLP\n",
        "\n",
        "需要的话我可以继续把多模态版补齐。\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}