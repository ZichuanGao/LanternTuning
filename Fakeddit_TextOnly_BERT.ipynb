{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fakeddit 文本‑only（BERT）基线\n",
        "\n",
        "本 Notebook 按论文方法论的**文本分支**做一个可复现的 baseline：\n",
        "- 使用 **Fakeddit v2.0 的 `multimodal_only_samples`**（仅图文样本，但这里只喂文本）\n",
        "- 文本使用 **`clean_title`**\n",
        "- 模型使用 **BERT** 做 2/3/6‑way 分类\n",
        "\n",
        "后续你可以在此基础上增加图像分支做多模态融合。\n",
        "\n",
        "**注意：**第一次运行需要下载 HuggingFace 模型权重（`bert-base-uncased`），请确保网络可用或已缓存。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0. 环境依赖（如本地未安装）\n",
        "如果你还没有安装依赖，可以在终端或此 Notebook 中执行：\n",
        "```bash\n",
        "pip install torch torchvision torchaudio transformers scikit-learn tqdm\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# 配置区（按需修改）\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "DATA_ROOT = Path('Fakeddit datasetv2.0')\n",
        "TRAIN_PATH = DATA_ROOT / 'multimodal_only_samples' / 'multimodal_train.tsv'\n",
        "VAL_PATH = DATA_ROOT / 'multimodal_only_samples' / 'multimodal_validate.tsv'\n",
        "TEST_PATH = DATA_ROOT / 'multimodal_only_samples' / 'multimodal_test_public.tsv'\n",
        "\n",
        "TASK = 2  # 2 / 3 / 6\n",
        "MODEL_NAME = 'bert-base-uncased'  # 有GPU可直接用\n",
        "MAX_LEN = 128\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 3\n",
        "LR = 2e-5\n",
        "WEIGHT_DECAY = 0.01\n",
        "WARMUP_RATIO = 0.1\n",
        "MAX_SAMPLES = None  # None 表示使用全部样本；调小可快速验证\n",
        "NUM_WORKERS = 2\n",
        "\n",
        "OUTPUT_DIR = Path('outputs') / f'bert_text_only_{TASK}way'\n",
        "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print('Train:', TRAIN_PATH)\n",
        "print('Val:', VAL_PATH)\n",
        "print('Test:', TEST_PATH)\n",
        "print('Output:', OUTPUT_DIR)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import csv\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, get_linear_schedule_with_warmup\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
        "\n",
        "try:\n",
        "    from tqdm import tqdm\n",
        "except Exception:\n",
        "    def tqdm(x, **kwargs):\n",
        "        return x\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('device:', device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) 标签映射说明\n",
        "以下映射已在本地 TSV 中验证：\n",
        "- 2‑way：`1=True`，`0=Fake`\n",
        "- 3‑way：`0=True`，`1=Fake with true text`，`2=Fake with false text`\n",
        "- 6‑way：`0=True`，`1=Satire/Parody`，`2=False Connection`，`3=Imposter`，`4=Manipulated`，`5=Misleading`\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "LABEL_NAMES = {\n",
        "    2: {0: 'Fake', 1: 'True'},\n",
        "    3: {0: 'True', 1: 'Fake-TrueText', 2: 'Fake-FalseText'},\n",
        "    6: {0: 'True', 1: 'Satire/Parody', 2: 'False Connection', 3: 'Imposter', 4: 'Manipulated', 5: 'Misleading'},\n",
        "}\n",
        "\n",
        "def load_tsv(path, task=2, max_samples=None):\n",
        "    texts = []\n",
        "    labels = []\n",
        "    label_key = f'{task}_way_label'\n",
        "    with open(path, newline='', encoding='utf-8') as f:\n",
        "        reader = csv.DictReader(f, delimiter='\t')\n",
        "        for i, row in enumerate(reader):\n",
        "            text = (row.get('clean_title') or '').strip()\n",
        "            if not text:\n",
        "                continue\n",
        "            label_str = row.get(label_key)\n",
        "            if label_str is None or label_str == '':\n",
        "                continue\n",
        "            label = int(float(label_str))\n",
        "            texts.append(text)\n",
        "            labels.append(label)\n",
        "            if max_samples is not None and len(texts) >= max_samples:\n",
        "                break\n",
        "    return texts, labels\n",
        "\n",
        "train_texts, train_labels = load_tsv(TRAIN_PATH, task=TASK, max_samples=MAX_SAMPLES)\n",
        "val_texts, val_labels = load_tsv(VAL_PATH, task=TASK, max_samples=MAX_SAMPLES)\n",
        "test_texts, test_labels = load_tsv(TEST_PATH, task=TASK, max_samples=MAX_SAMPLES)\n",
        "\n",
        "print('train:', len(train_texts))\n",
        "print('val  :', len(val_texts))\n",
        "print('test :', len(test_texts))\n",
        "\n",
        "# label distribution\n",
        "from collections import Counter\n",
        "print('train label dist:', Counter(train_labels))\n",
        "print('val label dist  :', Counter(val_labels))\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts, labels):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.texts[idx], self.labels[idx]\n",
        "\n",
        "def collate_fn(batch):\n",
        "    texts, labels = zip(*batch)\n",
        "    enc = tokenizer(list(texts), padding=True, truncation=True, max_length=MAX_LEN, return_tensors='pt')\n",
        "    enc['labels'] = torch.tensor(labels, dtype=torch.long)\n",
        "    return enc\n",
        "\n",
        "train_ds = TextDataset(train_texts, train_labels)\n",
        "val_ds = TextDataset(val_texts, val_labels)\n",
        "test_ds = TextDataset(test_texts, test_labels)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, collate_fn=collate_fn)\n",
        "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, collate_fn=collate_fn)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "num_labels = TASK\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=num_labels)\n",
        "model.to(device)\n",
        "\n",
        "# class weights (optional, useful for 6-way imbalance)\n",
        "from collections import Counter\n",
        "label_counts = Counter(train_labels)\n",
        "weights = [0.0] * num_labels\n",
        "total = sum(label_counts.values())\n",
        "for k in range(num_labels):\n",
        "    # inverse frequency\n",
        "    count = label_counts.get(k, 1)\n",
        "    weights[k] = total / (num_labels * count)\n",
        "weights = torch.tensor(weights, dtype=torch.float, device=device)\n",
        "\n",
        "# Replace loss to use class weights\n",
        "import torch.nn as nn\n",
        "criterion = nn.CrossEntropyLoss(weight=weights)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
        "total_steps = len(train_loader) * EPOCHS\n",
        "warmup_steps = int(total_steps * WARMUP_RATIO)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)\n",
        "\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=torch.cuda.is_available())\n",
        "print('total steps:', total_steps, 'warmup:', warmup_steps)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    total_loss = 0.0\n",
        "    for batch in loader:\n",
        "        labels = batch.pop('labels').to(device)\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        outputs = model(**batch)\n",
        "        logits = outputs.logits\n",
        "        loss = criterion(logits, labels)\n",
        "        total_loss += loss.item() * labels.size(0)\n",
        "        preds = torch.argmax(logits, dim=-1)\n",
        "        all_preds.extend(preds.cpu().tolist())\n",
        "        all_labels.extend(labels.cpu().tolist())\n",
        "\n",
        "    acc = accuracy_score(all_labels, all_preds)\n",
        "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "    avg_loss = total_loss / max(1, len(all_labels))\n",
        "    return avg_loss, acc, f1, cm\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "for epoch in range(1, EPOCHS + 1):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for batch in tqdm(train_loader, desc=f'Epoch {epoch}/{EPOCHS}'):\n",
        "        labels = batch.pop('labels').to(device)\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        optimizer.zero_grad()\n",
        "        with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
        "            outputs = model(**batch)\n",
        "            logits = outputs.logits\n",
        "            loss = criterion(logits, labels)\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        scheduler.step()\n",
        "        running_loss += loss.item() * labels.size(0)\n",
        "\n",
        "    train_loss = running_loss / max(1, len(train_ds))\n",
        "    val_loss, val_acc, val_f1, val_cm = evaluate(model, val_loader)\n",
        "    print(f'Epoch {epoch}: train_loss={train_loss:.4f} val_loss={val_loss:.4f} val_acc={val_acc:.4f} val_f1={val_f1:.4f}')\n",
        "    print('Val confusion matrix:\n', val_cm)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "test_loss, test_acc, test_f1, test_cm = evaluate(model, test_loader)\n",
        "print(f'Test: loss={test_loss:.4f} acc={test_acc:.4f} f1={test_f1:.4f}')\n",
        "print('Test confusion matrix:\n', test_cm)\n",
        "\n",
        "# 保存模型\n",
        "model.save_pretrained(OUTPUT_DIR)\n",
        "tokenizer.save_pretrained(OUTPUT_DIR)\n",
        "print('Saved to', OUTPUT_DIR)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 后续多模态融合建议\n",
        "- 保留这份文本‑only 作为基线\n",
        "- 添加图像分支（ResNet/ViT）并抽取图像向量\n",
        "- 融合策略：\n",
        "  - 拼接（concat）\n",
        "  - 模间注意力（cross‑attention）\n",
        "  - 语义一致性分支（cosine 或 MLP 预测一致性分数）\n",
        "- 最终输入：`[text_emb, image_emb, consistency_score]`\n",
        "\n",
        "如果你需要，我可以在此 Notebook 基础上继续扩展多模态版本。\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}